{"cells":[{"cell_type":"markdown","source":["### Spark session configuration\n","This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"92525f9d-94c0-4c57-9889-5de604aeac5a"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation.\n","# Licensed under the MIT License.\n","\n","spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"130188a5-6608-4480-97e7-d80e27ce6821","normalized_state":"finished","queued_time":"2025-04-05T16:02:53.2861099Z","session_start_time":null,"execution_start_time":"2025-04-05T16:02:53.2872946Z","execution_finish_time":"2025-04-05T16:02:53.582517Z","parent_msg_id":"aa9e88ea-7f4e-47ec-966c-f80c3533f308"},"text/plain":"StatementMeta(, 130188a5-6608-4480-97e7-d80e27ce6821, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0196844Z\",\"session_start_time\":\"2023-04-14T17:53:12.2618501Z\",\"execution_start_time\":\"2023-04-14T17:53:22.8437076Z\",\"execution_finish_time\":\"2023-04-14T17:53:24.8678387Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"},\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-05T16:02:53.2872946Z\",\"execution_finish_time\":\"2025-04-05T16:02:53.582517Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c086a1f-7fd5-4570-b76b-9deb49c52142"},{"cell_type":"markdown","source":["### Fact - Sale\n","\n","This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"850854f7-8c3e-4149-9bb4-891e16cde069"},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, quarter\n","\n","table_name = 'fact_sale'\n","\n","# Update the format to CSV and set options for header and schema inference\n","df = spark.read.format(\"csv\") \\\n","    .option(\"header\", \"true\") \\\n","    .option(\"inferSchema\", \"true\") \\\n","    .load('Files/0e7c2ef8-0e90-4aa3-8f71-551c25cc3016/WideWorldImportersDW/csv/full/fact_sale_1y_full')\n","\n","df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n","df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n","df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\", \"Quarter\").save(\"Tables/\" + table_name)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"130188a5-6608-4480-97e7-d80e27ce6821","normalized_state":"finished","queued_time":"2025-04-05T16:02:53.403301Z","session_start_time":null,"execution_start_time":"2025-04-05T16:02:53.5844569Z","execution_finish_time":"2025-04-05T16:05:49.3416576Z","parent_msg_id":"e0f3e80e-2633-44d3-9349-4a5d43f72316"},"text/plain":"StatementMeta(, 130188a5-6608-4480-97e7-d80e27ce6821, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T18:02:25.4147613Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T18:02:25.768228Z\",\"execution_finish_time\":\"2023-04-14T18:03:21.4120056Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"},\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-05T16:02:53.5844569Z\",\"execution_finish_time\":\"2025-04-05T16:05:49.3416576Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9375da35-d41b-4bfd-b460-05bf462b2637"},{"cell_type":"markdown","source":["### Dimensions\n","This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"618ad176-85ad-4771-a252-3711b02eca78"},{"cell_type":"code","source":["def loadFullDataFromSource(table_name):\n","    path = \"Files/0e7c2ef8-0e90-4aa3-8f71-551c25cc3016/WideWorldImportersDW/csv/full/\" + table_name\n","    # Read CSV with header and schema inference\n","    df = (\n","        spark.read\n","             .format(\"csv\")\n","             .option(\"header\", \"true\")\n","             .option(\"inferSchema\", \"true\")\n","             .load(path)\n","    )\n","    \n","    # Special case for dimension_customer which seems to have a more complex duplication issue\n","    if table_name == 'dimension_customer':\n","        # List all columns and manually rename them to ensure uniqueness\n","        print(f\"Column names for {table_name}: {df.columns}\")\n","        \n","        # Select each column with explicit aliases to avoid duplicates\n","        columns_with_aliases = [f\"`{col}` as `{col}_{i}`\" for i, col in enumerate(df.columns)]\n","        select_expr = \", \".join(columns_with_aliases)\n","        df = df.selectExpr(*columns_with_aliases)\n","    else:\n","        # Standard duplicate column handling for other tables\n","        cols = df.columns\n","        new_cols = []\n","        seen = {}\n","        for name in cols:\n","            if name in seen:\n","                seen[name] += 1\n","                new_cols.append(f\"{name}_{seen[name]}\")\n","            else:\n","                seen[name] = 0\n","                new_cols.append(name)\n","        \n","        # Rename DataFrame columns if duplicates were found\n","        if new_cols != cols:\n","            print(f\"Renaming columns for table '{table_name}':\")\n","            print(\"Original:\", cols)\n","            print(\"New:     \", new_cols)\n","            df = df.toDF(*new_cols)\n","    \n","    # Write the DataFrame to Delta format\n","    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n","\n","# List of dimension tables to ingest\n","dimension_tables = [\n","    'dimension_city',\n","    'dimension_customer',\n","    'dimension_date',\n","    'dimension_employee',\n","    'dimension_stock_item'\n","]\n","\n","for table in dimension_tables:\n","    try:\n","        loadFullDataFromSource(table)\n","        print(f\"Successfully loaded {table}\")\n","    except Exception as e:\n","        print(f\"Error loading {table}: {str(e)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:35:34.3075102Z","session_start_time":null,"execution_start_time":"2025-04-06T13:35:34.3088181Z","execution_finish_time":"2025-04-06T13:35:50.6341568Z","parent_msg_id":"c3d5fb11-5a36-4f6b-8f34-23d2a2f0b51d"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Successfully loaded dimension_city\nColumn names for dimension_customer: ['CustomerKey', 'WWICustomerID', 'Customer', 'BillToCustomer', 'Category', 'BuyingGroup', 'PrimaryContact', 'PostalCode', 'ValidFrom', 'ValidTo', 'LineageKey']\nError loading dimension_customer: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: f128e820-453c-487a-9d79-2d4335ba0d89).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- CustomerKey: long (nullable = true)\n-- WWICustomerID: long (nullable = true)\n-- Customer: string (nullable = true)\n-- BillToCustomer: string (nullable = true)\n-- Category: string (nullable = true)\n-- BuyingGroup: string (nullable = true)\n-- PrimaryContact: string (nullable = true)\n-- PostalCode: string (nullable = true)\n-- ValidFrom: timestamp (nullable = true)\n-- ValidTo: timestamp (nullable = true)\n-- LineageKey: long (nullable = true)\n\n\nData schema:\nroot\n-- CustomerKey_0: integer (nullable = true)\n-- WWICustomerID_1: integer (nullable = true)\n-- Customer_2: string (nullable = true)\n-- BillToCustomer_3: string (nullable = true)\n-- Category_4: string (nullable = true)\n-- BuyingGroup_5: string (nullable = true)\n-- PrimaryContact_6: string (nullable = true)\n-- PostalCode_7: string (nullable = true)\n-- ValidFrom_8: timestamp (nullable = true)\n-- ValidTo_9: timestamp (nullable = true)\n-- LineageKey_10: integer (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         \nSuccessfully loaded dimension_date\nSuccessfully loaded dimension_employee\nSuccessfully loaded dimension_stock_item\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0270275Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T17:56:22.4495972Z\",\"execution_finish_time\":\"2023-04-14T17:56:32.6772207Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"},\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:35:34.3088181Z\",\"execution_finish_time\":\"2025-04-06T13:35:50.6341568Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56e00b43-17e7-4992-9c36-5248fb4b76da"},{"cell_type":"code","source":["# Register dimension tables\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_city AS SELECT * FROM delta.`Tables/dimension_city`\")\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_date AS SELECT * FROM delta.`Tables/dimension_date`\")\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_employee AS SELECT * FROM delta.`Tables/dimension_employee`\")\n","\n","# Register fact table\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.fact_sale AS SELECT * FROM delta.`Tables/fact_sale`\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:39:38.2458579Z","session_start_time":null,"execution_start_time":"2025-04-06T13:39:38.2471586Z","execution_finish_time":"2025-04-06T13:39:49.8940337Z","parent_msg_id":"eac98ad1-1d36-468f-a6c8-329d957d5d7c"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"AnalysisException","evalue":"[SCHEMA_NOT_FOUND] The schema `wwilakehouse` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Register dimension tables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE OR REPLACE VIEW wwilakehouse.dimension_city AS SELECT * FROM delta.`Tables/dimension_city`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE OR REPLACE VIEW wwilakehouse.dimension_date AS SELECT * FROM delta.`Tables/dimension_date`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE OR REPLACE VIEW wwilakehouse.dimension_employee AS SELECT * FROM delta.`Tables/dimension_employee`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_NOT_FOUND] The schema `wwilakehouse` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS."]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:39:38.2471586Z\",\"execution_finish_time\":\"2025-04-06T13:39:49.8940337Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"},"id":"6a3f5c1b-774e-4090-a63c-bbf404c8046e"},{"cell_type":"code","source":["# First create the schema\n","spark.sql(\"CREATE SCHEMA IF NOT EXISTS wwilakehouse\")\n","\n","# Then register dimension tables\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_city AS SELECT * FROM delta.`Tables/dimension_city`\")\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_date AS SELECT * FROM delta.`Tables/dimension_date`\")\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.dimension_employee AS SELECT * FROM delta.`Tables/dimension_employee`\") \n","\n","# Register fact table\n","spark.sql(\"CREATE OR REPLACE VIEW wwilakehouse.fact_sale AS SELECT * FROM delta.`Tables/fact_sale`\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:40:13.1079577Z","session_start_time":null,"execution_start_time":"2025-04-06T13:40:13.1091015Z","execution_finish_time":"2025-04-06T13:40:13.9253735Z","parent_msg_id":"dabe3be2-2b5a-42a3-8aa8-5c7dd18ee393"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o341.sql.\n: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:275)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:309)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:327)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createNamespace(DelegatingCatalogExtension.java:163)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:148)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:231)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:272)\n\t... 46 more\nCaused by: java.lang.RuntimeException: Feature not supported on Apache Spark in Microsoft Fabric. Provided context: { spark.trident.pbiHost=api.fabric.microsoft.com, fs.defaultFS=abfss://125317d8-020f-4416-b744-c2611ae9d283@onelake.dfs.fabric.microsoft.com/, trident.capacity.id=2005b2a4-a182-4d80-9c05-36d0b7f0a3c9, spark.fabric.environmentDetails={}, trident.operation.type=SessionCreation, trident.workspace.id=125317d8-020f-4416-b744-c2611ae9d283, trident.tokenservice.zkcache.enabled=*****, trident.catalog.metastore.workspaceId=125317d8-020f-4416-b744-c2611ae9d283, spark.fabric.pools.category=Starter, spark.fabric.pools.poolHitEventTime=2025-04-06T13:34:24.6954002Z, trident.artifact.workspace.id=125317d8-020f-4416-b744-c2611ae9d283, trident.activity.id=e83de5de-6890-4da9-9b6e-eb734c0fdd75, spark.trident.lineage.enabled=false, trident.artifact.type=SynapseNotebook, trident.lineage.enabled=False, trident.materializedview.libraries.enabled=false, trident.lakehouse.tokenservice.endpoint=htt***ken, spark.trident.disable_autolog=false, fs.homeDir=/b8977ecf-a27a-4dfe-9042-39d9955a0d83, spark.fabric.pool.name=Starter Pool, spark.synapse.nbs.session.timeout=1200000, trident.tenant.id=f74b1450-e46a-41df-abee-ebf3621bfd85, trident.esri.libraries.enabled=false, trident.catalog.metastore.lakehouseName=wwi_lakehouse, spark.fabric.resourceProfile=writeHeavy, spark.trident.autotune.fetchSAS.url=https://pbipncus19-northcentralus.pbidedicated.windows.net/webapi/capacities/2005b2a4-a182-4d80-9c05-36d0b7f0a3c9/workloads/SparkCore/SparkCoreService/automatic/v1/autotune/fetchAutotuneStorageSasUrl, trident.moniker.id=e83de5de-6890-4da9-9b6e-eb734c0fdd75, spark.trident.highconcurrency.enabled=false, spark.trident.session.submittedAt=1743946464768, trident.artifact.id=f5bb1cf2-8900-4ca2-9f01-0aba84e048a0, spark.cluster.type=trident, spark.synapse.context.notebookname=01-Create-Delta-Tables, spark.fabric.pools.vhdOverride=false, spark.fabric.pools.poolHit=true, trident.lakehouse.name=wwi_lakehouse, trident.lakehouse.id=b8977ecf-a27a-4dfe-9042-39d9955a0d83, spark.synapse.nbs.kernelid=63d4f92c-7090-402b-9b02-13d94b122e0f, trident.session.token=eyJ***DMQ }\n\tat com.microsoft.azure.trident.core.TridentHelper.failIfValidTridentContext(TridentHelper.java:249)\n\t... 51 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# First create the schema\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE SCHEMA IF NOT EXISTS wwilakehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Then register dimension tables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE OR REPLACE VIEW wwilakehouse.dimension_city AS SELECT * FROM delta.`Tables/dimension_city`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o341.sql.\n: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:275)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:309)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.createNamespace(V2SessionCatalog.scala:327)\n\tat org.apache.spark.sql.connector.catalog.DelegatingCatalogExtension.createNamespace(DelegatingCatalogExtension.java:163)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:160)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:148)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:33)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:126)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:231)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat com.microsoft.azure.trident.spark.TridentCoreProxy.failCreateDbIfTrident(TridentCoreProxy.java:272)\n\t... 46 more\nCaused by: java.lang.RuntimeException: Feature not supported on Apache Spark in Microsoft Fabric. Provided context: { spark.trident.pbiHost=api.fabric.microsoft.com, fs.defaultFS=abfss://125317d8-020f-4416-b744-c2611ae9d283@onelake.dfs.fabric.microsoft.com/, trident.capacity.id=2005b2a4-a182-4d80-9c05-36d0b7f0a3c9, spark.fabric.environmentDetails={}, trident.operation.type=SessionCreation, trident.workspace.id=125317d8-020f-4416-b744-c2611ae9d283, trident.tokenservice.zkcache.enabled=*****, trident.catalog.metastore.workspaceId=125317d8-020f-4416-b744-c2611ae9d283, spark.fabric.pools.category=Starter, spark.fabric.pools.poolHitEventTime=2025-04-06T13:34:24.6954002Z, trident.artifact.workspace.id=125317d8-020f-4416-b744-c2611ae9d283, trident.activity.id=e83de5de-6890-4da9-9b6e-eb734c0fdd75, spark.trident.lineage.enabled=false, trident.artifact.type=SynapseNotebook, trident.lineage.enabled=False, trident.materializedview.libraries.enabled=false, trident.lakehouse.tokenservice.endpoint=htt***ken, spark.trident.disable_autolog=false, fs.homeDir=/b8977ecf-a27a-4dfe-9042-39d9955a0d83, spark.fabric.pool.name=Starter Pool, spark.synapse.nbs.session.timeout=1200000, trident.tenant.id=f74b1450-e46a-41df-abee-ebf3621bfd85, trident.esri.libraries.enabled=false, trident.catalog.metastore.lakehouseName=wwi_lakehouse, spark.fabric.resourceProfile=writeHeavy, spark.trident.autotune.fetchSAS.url=https://pbipncus19-northcentralus.pbidedicated.windows.net/webapi/capacities/2005b2a4-a182-4d80-9c05-36d0b7f0a3c9/workloads/SparkCore/SparkCoreService/automatic/v1/autotune/fetchAutotuneStorageSasUrl, trident.moniker.id=e83de5de-6890-4da9-9b6e-eb734c0fdd75, spark.trident.highconcurrency.enabled=false, spark.trident.session.submittedAt=1743946464768, trident.artifact.id=f5bb1cf2-8900-4ca2-9f01-0aba84e048a0, spark.cluster.type=trident, spark.synapse.context.notebookname=01-Create-Delta-Tables, spark.fabric.pools.vhdOverride=false, spark.fabric.pools.poolHit=true, trident.lakehouse.name=wwi_lakehouse, trident.lakehouse.id=b8977ecf-a27a-4dfe-9042-39d9955a0d83, spark.synapse.nbs.kernelid=63d4f92c-7090-402b-9b02-13d94b122e0f, trident.session.token=eyJ***DMQ }\n\tat com.microsoft.azure.trident.core.TridentHelper.failIfValidTridentContext(TridentHelper.java:249)\n\t... 51 more\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:40:13.1091015Z\",\"execution_finish_time\":\"2025-04-06T13:40:13.9253735Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"},"id":"20e8b213-8071-475d-959b-fc75cfd00311"},{"cell_type":"code","source":["# Use direct table references instead\n","df_fact_sale = spark.read.table(\"delta.`Tables/fact_sale`\") \n","df_dimension_date = spark.read.table(\"delta.`Tables/dimension_date`\")\n","df_dimension_city = spark.read.table(\"delta.`Tables/dimension_city`\")\n","df_dimension_employee = spark.read.table(\"delta.`Tables/dimension_employee`\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:40:48.4009655Z","session_start_time":null,"execution_start_time":"2025-04-06T13:40:48.402094Z","execution_finish_time":"2025-04-06T13:40:54.440641Z","parent_msg_id":"534e43de-eb42-4135-8aed-349fc3a4e462"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:40:48.402094Z\",\"execution_finish_time\":\"2025-04-06T13:40:54.440641Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"},"id":"473f94b0-4a77-4c7c-99a7-38e5d6df143a"},{"cell_type":"code","source":["# Create temporary views\n","df_fact_sale.createOrReplaceTempView(\"fact_sale\")\n","df_dimension_date.createOrReplaceTempView(\"dimension_date\")\n","df_dimension_city.createOrReplaceTempView(\"dimension_city\")\n","df_dimension_employee.createOrReplaceTempView(\"dimension_employee\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:41:00.4173099Z","session_start_time":null,"execution_start_time":"2025-04-06T13:41:00.4185056Z","execution_finish_time":"2025-04-06T13:41:00.7132665Z","parent_msg_id":"19557684-ade3-409e-b153-32f67b133fa7"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:41:00.4185056Z\",\"execution_finish_time\":\"2025-04-06T13:41:00.7132665Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"},"id":"e2b4ec15-8a20-4685-8ea2-d900054776ea"},{"cell_type":"code","source":["%%sql\n","CREATE OR REPLACE TEMPORARY VIEW sale_by_date_employee\n","AS\n","SELECT\n","    DD.Date, DD.CalendarMonthLabel\n","    , DD.Day, DD.ShortMonth Month, CalendarYear Year\n","    ,DE.PreferredName, DE.Employee\n","    ,SUM(FS.TotalExcludingTax) SumOfTotalExcludingTax\n","    ,SUM(FS.TaxAmount) SumOfTaxAmount\n","    ,SUM(FS.TotalIncludingTax) SumOfTotalIncludingTax\n","    ,SUM(Profit) SumOfProfit \n","FROM fact_sale FS\n","INNER JOIN dimension_date DD ON FS.InvoiceDateKey = DD.Date\n","INNER JOIN dimension_employee DE ON FS.SalespersonKey = DE.EmployeeKey\n","GROUP BY DD.Date, DD.CalendarMonthLabel, DD.Day, DD.ShortMonth, DD.CalendarYear, DE.PreferredName, DE.Employee\n","ORDER BY DD.Date ASC, DE.PreferredName ASC, DE.Employee ASC"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"e83de5de-6890-4da9-9b6e-eb734c0fdd75","normalized_state":"finished","queued_time":"2025-04-06T13:41:13.1574319Z","session_start_time":null,"execution_start_time":"2025-04-06T13:41:13.1587044Z","execution_finish_time":"2025-04-06T13:41:13.8868372Z","parent_msg_id":"93d33445-56ce-4d74-8899-7010b38d93a7"},"text/plain":"StatementMeta(, e83de5de-6890-4da9-9b6e-eb734c0fdd75, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":7,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[]},"data":[]},"text/plain":"<Spark SQL result set with 0 rows and 0 fields>"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"cellStatus":"{\"Tim Warner\":{\"session_start_time\":null,\"execution_start_time\":\"2025-04-06T13:41:13.1587044Z\",\"execution_finish_time\":\"2025-04-06T13:41:13.8868372Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","collapsed":false},"id":"a5683973-4c7b-4e50-a0a3-af503fb46322"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"notebook_environment":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b8977ecf-a27a-4dfe-9042-39d9955a0d83"}],"default_lakehouse":"b8977ecf-a27a-4dfe-9042-39d9955a0d83","default_lakehouse_name":"wwi_lakehouse","default_lakehouse_workspace_id":"125317d8-020f-4416-b744-c2611ae9d283"}}},"nbformat":4,"nbformat_minor":5}