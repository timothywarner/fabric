{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8dcdbf-9f10-462a-959c-3ef143d496ba",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 6: Batch inferencing with PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0a7d5-a70b-42a7-8ace-f24e11b619e8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Microsoft Fabric allows users to operationalize machine learning models with a scalable function called PREDICT, which supports batch scoring in any compute engine. Users can generate batch predictions directly from a Microsoft Fabric notebook or from a given ML model's item page.\n",
    "\n",
    "### Exercise overview\n",
    "\n",
    "In this exercise, you learn how to apply PREDICT both ways, whether you're more comfortable writing code yourself or using a guided UI experience to handle batch scoring for you.\n",
    "\n",
    "### Helpful links\n",
    "- [PREDICT in Microsoft Fabric](https://aka.ms/fabric-predict) \n",
    "\n",
    "### Limitations \n",
    "\n",
    "Note that the PREDICT function is currently supported for a limited set of ML model flavors, including:\n",
    "- PyTorch\n",
    "- Sklearn\n",
    "- Spark\n",
    "- TensorFlow\n",
    "- ONNX\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "- Statsmodels\n",
    "- Prophet\n",
    "- Keras\n",
    "- PREDICT ***requires*** ML models to be saved in the MLflow format with their signatures populated.\n",
    "- PREDICT ***does*** not support ML models with multi-tensor inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69232a-bb01-4449-bbea-1e57a6351b72",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Pre-Requisite\n",
    "\n",
    "For this Exercise, we expect that you have completed and ran **Labs 1-4**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b44c72-170b-4ac1-a9cf-ffe017ed59f7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Setup your notebook\n",
    "\n",
    "### Select Lakehouse\n",
    "\n",
    "First, add the Lakehouse you created from the prior lab exercise.\n",
    "\n",
    "<br>\n",
    "\n",
    "![image-alt-text](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/add-lakehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f291bb1-3897-4da2-a7ea-76a4549f801a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Load test data as a Spark DataFrame\n",
    "\n",
    "To generate batch predictions using an already trained ML model (in this case version 1 of the churn model in previous notebook), you need the test data in the form of a Spark DataFrame.\n",
    "\n",
    "Load the test data that was stored as a Lakehouse table during training back into a Spark DataFrame in order to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a607af-156c-47d5-b0a4-7c78c6c90098",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.format(\"delta\").load(\"Tables/churn_test_data\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99441a0-c04d-472e-8724-e1ed11745da0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Generate PREDICT code from an ML model's item page\n",
    "\n",
    "From any ML model's item page, you can choose either of the following options to start generating batch predictions for a specific model version with PREDICT.\n",
    "\n",
    "- Use a guided UI experience to generate PREDICT code\n",
    "- Copy a code template into a notebook and customize the parameters yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a1d28-af9a-45b2-9a0f-15aca7b106d9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Use a guided UI experience\n",
    "\n",
    "The guided UI experience walks you through steps to:\n",
    "\n",
    "- Select source data for scoring\n",
    "- Map the data correctly to your ML model's inputs\n",
    "- Specify the destination for your model's outputs\n",
    "- Create a notebook that uses PREDICT to generate and store prediction results\n",
    "\n",
    "To use the guided experience,\n",
    "\n",
    "1. Go to the item page for a given ML model version.\n",
    "\n",
    "2. Select **Apply this model in wizard** from the **Apply this version** dropdown.\n",
    "\n",
    "The selection opens up the \"Apply ML model predictions\" window at the \"Select input table\" step.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/1.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d3981-cbb9-4b02-85c8-7eca311d192d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "3. Select an input table from one of the lakehouses in your current workspace.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/2.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f5765-2dce-4dbb-a5b8-17919a087796",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "4. Select Next to go to the \"Map input columns\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8b984-dc4a-4821-9159-460cd4db2219",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "5. Map column names from the source table to the ML model's input fields, which are pulled from the model's signature. You must provide an input column for all the model's required fields. Also, the data types for the source columns must match the model's expected data types.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/3.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b397cf-da5f-4fde-a081-a765e0216879",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "6. Select Next to go to the \"Create output table\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10c582-30a0-47ea-b8ab-31fc70438eef",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "7. Provide a name for a new table within the selected lakehouse of your current workspace. This output table stores your ML model's input values with the prediction values appended. By default, the output table is created in the same lakehouse as the input table, but the option to change the destination lakehouse is also available.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/4.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbce78-e90d-49cc-8384-b0e7c60fb9e5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "8. Select Next to go to the \"Map output columns\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2dc99-c8e8-47c3-abe4-fdefe24a06da",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "9. Use the provided text fields to name the columns in the output table that stores the ML model's predictions.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/5.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b19504-6c28-441f-8183-d16824a12d12",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "10. Select **Next** to go to the \"Configure notebook\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b3180-5df3-4ee9-8705-1875b48b59e2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "11. Provide a name for a new notebook that will run the generated PREDICT code. The wizard displays a preview of the generated code at this step. You can copy the code to your clipboard and paste it into an existing notebook if you prefer.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/6.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65577f6a-9af0-43a1-8c21-9e4031dc4d50",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "12. Select **Next** to go to the \"Review and finish\" step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cf07b-a70c-4924-80cf-e7e55a1619b6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "13. Review the details on the summary page and select **Create notebook** to add the new notebook with its generated code to your workspace. You're taken directly to that notebook, where you can run the code to generate and store predictions.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/Predict/7.png\"  width=\"400%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3dc43-0a6d-4266-ab24-8d976fa620d6",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Use a customizable code template\n",
    "\n",
    "To generate predictions using the PREDICT function without using the UI, you can use the Transformer API, the Spark SQL API, or a PySpark user-defined function (UDF). The following sections show how to generate batch predictions with the test data and the trained ML model, using the different methods for invoking PREDICT. Note that you need to manually replace the following values:\n",
    "\n",
    "- `<INPUT_COLS>`: An array of column names from the input table to feed to the ML model\n",
    "- `<OUTPUT_COLS>`: A name for a new column in the output table that stores predictions\n",
    "- `<MODEL_NAME>`: The name of the ML model to use for generating predictions\n",
    "- `<MODEL_VERSION>`: The version of the ML model to use for generating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ad461-5d78-4bf0-a076-17c135287995",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### PREDICT with the Transformer API\n",
    "\n",
    "PREDICT supports MLflow-packaged models in the Microsoft Fabric registry. Therefore, to use the Transformer API from SynapseML, you'll need to first create an MLFlowTransformer object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd699-aee0-4bf3-a34f-aebb7d24678e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Instantiate MLFlowTransformer object\n",
    "\n",
    "The MLFlowTransformer object is a wrapper around the MLFlow model that you have already registered. It allows you to generate batch predictions on a given DataFrame. To instantiate the MLFlowTransformer object, you'll need to provide the following parameters:\n",
    "\n",
    "- The columns from the test DataFrame that you need as input to the model (in this case, you would need all of them).\n",
    "- A name for the new output column (in this case, `predictions`).\n",
    "- The correct model name and model version to generate the predictions (in this case, `churn-model` and version 1).\n",
    "\n",
    "If you've been using your own ML model, substitute the values for the `model` and `test data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7529aa2c-f4d1-47ac-be01-e4d44981136a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.predict import MLFlowTransformer\n",
    "\n",
    "model = MLFlowTransformer(\n",
    "    inputCols=list(df_test.columns),\n",
    "    outputCol='predictions',\n",
    "    modelName='fabcon-churn-model',\n",
    "    modelVersion=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1b55d-a96a-4b22-854f-cb2f1183af2c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Now that you have the MLFlowTransformer object, you can use it to generate batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcddb96-fcc9-4d34-84cc-b218728a1489",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "predictions = model.transform(df_test)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b90d2-af56-4c0a-a4a2-dfddf09ad1f3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff80f7-8174-4a59-ad84-627368f2c230",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### PREDICT with the Spark SQL API\n",
    "\n",
    "The following code invokes the PREDICT function with the Spark SQL API. If you've been using your own ML model, substitute the values for `model_name`, `model_version`, and `features` with your model name, model version, and feature columns.\n",
    "\n",
    "> [!NOTE]\n",
    "> Using the Spark SQL API to generate predictions still requires you to create an MLFlowTransformer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1ba52-695d-4b64-bc3a-74ae1b529072",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer \n",
    "\n",
    "# Substitute \"model_name\", \"model_version\", and \"features\" below with values for your own model name, model version, and feature columns\n",
    "model_name = 'fabcon-churn-model'\n",
    "model_version = 1\n",
    "features = df_test.columns\n",
    "\n",
    "sqlt = SQLTransformer().setStatement( \n",
    "    f\"SELECT PREDICT('{model_name}/{model_version}', {','.join(features)}) as predictions FROM __THIS__\")\n",
    "\n",
    "# Substitute \"X_test\" below with your own test dataset\n",
    "display(sqlt.transform(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e329cb0-cc8c-404a-901e-043e85fb3e55",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### PREDICT with a user-defined function (UDF)\n",
    "\n",
    "The following code invokes the PREDICT function with a PySpark UDF. If you've been using your own ML model, substitute the values for the `model` and `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677d253-941d-4289-a368-288f0397c29c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, udf, lit\n",
    "\n",
    "# Substitute \"model\" and \"features\" below with values for your own model name and feature columns\n",
    "my_udf = model.to_udf()\n",
    "features = df_test.columns\n",
    "\n",
    "display(df_test.withColumn(\"predictions\", my_udf(*[col(f) for f in features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39577e37-e471-4d5f-abf5-4e82efd2dd11",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Write model prediction results to the lakehouse\n",
    "\n",
    "Once you have generated batch predictions, write the model prediction results back to the lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe73a8-dd6c-4b23-b39e-1db894740da0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions to lakehouse to be used for generating a Power BI report\n",
    "table_name = \"customer_churn_test_predictions\"\n",
    "predictions.write.format('delta').mode(\"overwrite\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark DataFrame saved to delta table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a053164-5423-4c78-9c08-ed5276e48b84",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39c9cc-9497-4b31-8fcc-86f6335e91b5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Business Intelligence via Visualizations in Power BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624e0be-ebb2-4585-852d-f562aba1fa93",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Exercise 1: Build a Power BI dashboard report\n",
    "\n",
    "Next, you'll analyze the saved prediction results in Power BI to build a dashboard to shed some lights on business insights that help with avoiding the retention of customers.\n",
    "\n",
    "### To do\n",
    "In this exercise, you will follow these instructions to build a Power BI report.\n",
    "\n",
    "> [!NOTE]\n",
    "> This shows an illustrated example of how you would analyze the saved prediction results in Power BI. However, for a real customer churn use-case, the platform user may have to do more thorough ideation of what visualizations to create, based on subject matter expertise, and what their firm and business analytics team has standardized as metrics.\n",
    "\n",
    "To access your saved table in Power BI:\n",
    "\n",
    "1. On the left, select **OneLake data hub**.\n",
    "2. Select the lakehouse that you added to this notebook.\n",
    "3. On the top right, select **Open** under the section titled **Open this Lakehouse**.\n",
    "4. Select New Power BI dataset on the top ribbon and select `customer_churn_test_predictions`, then select **Continue** to create a new Power BI dataset linked to the predictions.\n",
    "5. On the tools at the top of the dataset page, select **New report** to open the Power BI report authoring page.\n",
    "\n",
    "Some example visualizations are shown here. The data panel shows the delta tables and columns from the table to select. Upon selecting appropriate x and y axes, you can pick the filters and functions, for example, sum or average of the table column.\n",
    "\n",
    "## Create a semantic model\n",
    "\n",
    "Create a new semantic model linked to the predictions data you produced in part 4:\n",
    "\n",
    "1. On the left, select your workspace.\n",
    "2. On the top left, select **Lakehouse** as a filter.\n",
    "3. Select the lakehouse that you used in the previous parts of the tutorial series.\n",
    "4. Select **New semantic model** on the top ribbon.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/new-power-bi-dataset.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "1. Give the semantic model a name, such as \"bank churn predictions.\" Then select the **customer_churn_test_predictions** dataset.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/select-predictions-data.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "2. Select **Confirm**.  \n",
    "\n",
    "## Add new measures\n",
    "\n",
    "Now add a few measures to the semantic model:\n",
    "\n",
    "3. Add a new measure for the churn rate.\n",
    "\n",
    "1. Select **New measure** in the top ribbon.  This action adds a new item named **Measure** to the **customer_churn_test_predictions** dataset, and opens a formula bar above the table.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/new-measure.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "2. To determine the average predicted churn rate, replace `Measure =` in the formula bar with:\n",
    "\n",
    "        `Churn Rate = AVERAGE(customer_churn_test_predictions[predictions])`\n",
    "    \n",
    "\n",
    "3. To apply the formula, select the check mark in the formula bar.  The new measure appears in the data table.  The calculator icon shows it was created as a measure.\n",
    "\n",
    "1. Change the format from **General** to **Percentage** in the **Properties** panel.\n",
    "2. Scroll down in the **Properties** panel to change the **Decimal places** to 1.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/churn-rate.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "4. Add a new measure that counts the total number of bank customers.  You'll need it for the rest of the new measures.\n",
    "  \n",
    "    1. Select **New measure** in the top ribbon to add a new item named **Measure** to the `customer_churn_test_predictions` dataset.  This action also opens a formula bar above the table.\n",
    "    2. Each prediction represents one customer. To determine the total number of customers, replace `Measure =` in the formula bar with:\n",
    "\n",
    "        \n",
    "        `Customers = COUNT(customer_churn_test_predictions[predictions])`\n",
    "        \n",
    "\n",
    "    3. Select the check mark in the formula bar to apply the formula.\n",
    "\n",
    "5. Add the churn rate for Germany.\n",
    "\n",
    "    1. Select **New measure** in the top ribbon to add a new item named **Measure** to the `customer_churn_test_predictions` dataset.  This action also opens a formula bar above the table.\n",
    "\n",
    "    2. To determine the churn rate for Germany, replace `Measure =` in the formula bar with:\n",
    "\n",
    "        `Germany Churn = CALCULATE(customer_churn_test_predictions[Churn Rate], customer_churn_test_predictions[Geography_Germany] = 1)`\n",
    "\n",
    "        This filters the rows down to the ones with Germany as their geography (Geography_Germany equals one).\n",
    "\n",
    "    3. To apply the formula, select the check mark in the formula bar.\n",
    "\n",
    "6. Repeat the above step to add the churn rates for France and Spain.\n",
    "\n",
    "    * Spain's churn rate:\n",
    "\n",
    "        ```python\n",
    "        Spain Churn = CALCULATE(customer_churn_test_predictions[Churn Rate], customer_churn_test_predictions[Geography_Spain] = 1)\n",
    "        ```\n",
    "\n",
    "    * France's churn rate:\n",
    "\n",
    "        ```python\n",
    "        France Churn = CALCULATE(customer_churn_test_predictions[Churn Rate], customer_churn_test_predictions[Geography_France] = 1)\n",
    "        ```\n",
    "\n",
    "## Create new report\n",
    "\n",
    "Once you're done with all operations, move on to the Power BI report authoring page by selecting **Create report** on the top ribbon.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/visualize-this-data.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "Once the report page appears, add these visuals:\n",
    "\n",
    "1. Select the text box on the top ribbon and enter a title for the report, such as \"Bank Customer Churn\".  Change the font size and background color in the Format panel.  Adjust the font size and color by selecting the text and using the format bar.\n",
    "\n",
    "2. In the Visualizations panel, select the **Card** icon. From the **Data** pane, select **Churn Rate**. Change the font size and background color in the Format panel. Drag this visualization to the top right of the report.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/card-churn.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "3. In the Visualizations panel, select the **Line and stacked column chart** icon. Select **age** for the x-axis, **Churn Rate** for column y-axis, and **Customers** for the line y-axis.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/age.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "4. In the Visualizations panel, select the **Line and stacked column chart** icon. Select **NumOfProducts** for x-axis, **Churn Rate** for column y-axis, and **Customers** for the line y-axis.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/number-of-products.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "5. In the Visualizations panel, select the **Stacked column chart** icon. Select **NewCreditsScore** for x-axis and  **Churn Rate** for y-axis.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/new-credit-score.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "    Change the title \"NewCreditsScore\" to \"Credit Score\" in the Format panel.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/change-title.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "6. In the Visualizations panel, select the **Clustered column chart** card. Select **Germany Churn**, **Spain Churn**, **France Churn** in that order for the y-axis.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/germany-spain-france.png\"  width=\"50%\" height=\"20%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n",
    "\n",
    "The Power BI report shows:\n",
    "\n",
    "* Customers who use more than two of the bank products have a higher churn rate although few customers had more than two products. The bank should collect more data, but also investigate other features correlated with more products (see the plot in the bottom left panel).\n",
    "* Bank customers in Germany have a higher churn rate than in France and Spain (see the plot in the bottom right panel), which suggests that an investigation into what has encouraged customers to leave could be beneficial.\n",
    "* There are more middle aged customers (between 25-45) and customers between 45-60 tend to exit more.\n",
    "* Finally, customers with lower credit scores would most likely leave the bank for other financial institutes. The bank should look into ways that encourage customers with lower credit scores and account balances to stay with the bank.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/PBI/germany-spain-france.png\"  width=\"100%\" height=\"100%\" title=\"Screenshot shows logged values for one of the models.\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8b70a-8749-424c-a89e-74495a4d6358",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce99eb9-4960-44d7-a970-da8900173960",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Bonus - Visualize the Power BI report in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc398442-513d-46cc-ab6f-a0e2bedd25b5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from powerbiclient import QuickVisualize, get_dataset_config\n",
    "\n",
    "df_predictions = spark.read.format(\"delta\").load(\"Tables/customer_churn_test_predictions\")\n",
    "\n",
    "PBI_visualize = QuickVisualize(get_dataset_config(df_predictions))\n",
    "\n",
    "# Render Power BI report in the notebook\n",
    "PBI_visualize"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
