{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56db62c-7ef1-4350-8761-58157f43019c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Lab 4B: Hyperparameter Tuning in Fabric\n",
    "\n",
    "Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning algorithm. Hyperparameters are parameters that are set before the learning process begins, such as the learning rate and the regularization parameter in a regression model. Tuning these hyperparameters requires exploring a specified range of values and assessing the model's performance with each combination. This process can be challenging and time-consuming, particularly when dealing with complex models and large datasets.\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/HyperparameterTuningFabric.png\" alt=\"Alt text\" height=\"400\" width=\"800\">\n",
    "\n",
    "### Exercise overview\n",
    "In this exercise, we will use `churn_data_clean` to tune a LightGBM model. We will track the results of these iterations with MLFlow.\n",
    "\n",
    "### Helpful links\n",
    "- [Autologging in Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/data-science/mlflow-autologging)\n",
    "- [Fabric Experiments](https://learn.microsoft.com/en-us/fabric/data-science/machine-learning-experiment)\n",
    "- [Tune Examples](https://microsoft.github.io/FLAML/docs/Examples/Tune-PyTorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b96c3-bc2d-484e-8b0e-382b579c57e8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Pre-Requisite\n",
    "\n",
    "For this Exercise, we expect that you have completed and ran **Lab 3: Data Preparation & Exploratory Data Analysis**. This will generate your cleaned data that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a9573-10dd-4997-8a8b-9cb52a159d3d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 0: Attach Lakehouse\n",
    "\n",
    "First, add the Lakehouse you created from the Lab 0.\n",
    "\n",
    "![image-alt-text](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/add-lakehouse.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c701ba-83fd-4500-b25d-b0b3440e3f8b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install Fabric integrated version of FLAML\n",
    "%pip install https://synapsemldatascience.blob.core.windows.net/releases/flaml/FLAML-2.1.1.post4-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8550ae2-a778-421e-8b6b-69ec0c94d11b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Load the data\n",
    "\n",
    "We load the prepped churn data for training, utilizing Apache Spark's DataFrame API to read data stored in Delta format from the specified location \"Tables/churn_data_clean\". The DataFrame `df_final` is then ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a2934-5a0e-493c-92cd-05a396e5ba32",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load the prepped data for churn\n",
    "df_final = spark.read.format(\"delta\").load(\"Tables/churn_data_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706114bd-8fc4-4d55-b7bf-3a40a1c3fcce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353ab68-846e-4525-b2aa-2237b556fa8e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53213d28-bb6c-4a97-abfa-6814ac2b1111",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- The required libraries are imported for model training, including `train_test_split` from Scikit-Learn.\n",
    "- The Spark DataFrame `df_final` is converted to a Pandas DataFrame `df_final_pd`, and X and Y datasets are defined.\n",
    "- Data is split into training and testing sets using the `train_test_split` function, with a test size of 20% and a specified random state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8af33-09a0-4660-a52d-22559fb0ad04",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the required libraries for model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to pandas and define X and Y dataset\n",
    "\n",
    "df_final_pd = df_final.toPandas()\n",
    "random_state = 41\n",
    "y = df_final_pd[\"Exited\"]\n",
    "X = df_final_pd.drop(\"Exited\",axis=1)\n",
    "\n",
    "# Train-Test Separation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3f691-ef7b-44cb-b692-e3a6afe8cf1c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Set up MLflow experiment tracking\n",
    "\n",
    "\n",
    "MLflow is an open source platform that is deeply integrated into the Data Science experience in Fabric and allows to easily track and compare the performance of different models and experiments without the need for manual tracking.You can learn more about MLFlow [from the MLFlow documentation](https://mlflow.org/docs/2.4.2/what-is-mlflow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e1e87-cfa1-4706-bbf6-456697a59a4c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import MLflow and set up the experiment name\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"FabCon-Demo-Experiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea7170-af75-4d10-a79d-52e3c53feace",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Train the baseline model\n",
    "\n",
    "In this code cell, a baseline ```LightGBM``` model is trained using arbitrary parameters, without optimization for performance. This serves as an initial benchmark for comparison with more refined models. By establishing this baseline, subsequent iterations or parameter tuning efforts can be evaluated in terms of their effectiveness in improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5549ec-54d1-4ed1-8dbb-0a295ae62c46",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Set the logging level\n",
    "\n",
    "You can configure the logging level to suppress unnecessary outputs to keep the logs cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9d158-8965-4798-9a76-8030cfc35d44",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    " \n",
    "logging.getLogger('synapse.ml').setLevel(logging.ERROR)\n",
    "logging.getLogger('mlflow.utils').setLevel(logging.ERROR)\n",
    "logging.getLogger('mlflow.utils.autologging_utils').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2178e7-9691-422f-9022-c16517b8fcb4",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### View the performance of the baseline model\n",
    "\n",
    "This cell establishes a baseline for model performance and enables tracking of experiment results for analysis and comparison. This cell does the following steps:\n",
    "\n",
    "1. The code cell begins by initiating an MLflow run named ```default``` to track the experiment. \n",
    "1. A ```LightGBM``` classifier is then defined with predetermined parameters. The model is trained on the provided training data, and subsequent predictions are made on the test dataset. \n",
    "1. Performance metrics such as ```accuracy``` and ```ROC AUC``` score are computed and logged using MLflow. The run ID is captured for future reference. \n",
    "\n",
    "You can leverage the MLflow notebook widget to explore the properties, metrics, and parameters of the generated run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b32332-84fb-44a4-825e-8da662871196",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import mlflow\n",
    "import logging\n",
    "\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"default\") as run:\n",
    "    # Define LGBMClassifier with specified parameters\n",
    "    default_model = LGBMClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=2,\n",
    "        max_depth=2,\n",
    "        num_leaves=3,\n",
    "        objective='binary',\n",
    "        random_state=random_state,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    # Capture run_id for model prediction later\n",
    "    default_model_run_id = run.info.run_id \n",
    "\n",
    "    # Fit the model to the training data\n",
    "    default_model.fit(X_train, y_train) \n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = default_model.predict(X_test)\n",
    "    \n",
    "\n",
    "    # Compute ROC AUC score\n",
    "    roc_auc_default = roc_auc_score(y_train, default_model.predict_proba(X_train)[:, 1])\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_default)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c66c91-1726-419c-a919-d4068eb9d2ff",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 4: Perform hyperparameter tuning with FLAML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caded8a-59a8-4256-a998-bcaf1ca8798d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Disable MLflow autologging to leverage FLAML tune's built-in logging\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69571a-4a91-450a-add4-2c904bd10464",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "FLAML is a fast and lightweight AutoML library that can automatically find the optimal hyperparameters for a given model and dataset. It uses a low-cost search strategy that adapts to the feedback from the evaluation metrics. In this section, you will utilize FLAML to fine-tune the hyperparameters of the LightGBM model defined in the previous section.\n",
    "\n",
    "\n",
    "<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/tune_parameters.png\" alt=\"Tune LightGBM parameters\" height=\"400\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f557211-c08c-4072-af20-1e65d15039b5",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define the ```train``` function\n",
    "\n",
    "You then need to define the train function, which requires four hyperparameters as inputs: `n_estimators`, `learningRate`, and `numLeaves`. Note that these hyperparameters will be tuned later using FLAML.\n",
    "\n",
    "Additionally, the train function expects two DataFrames as inputs: `train_data` and `val_data`, representing the training and test datasets, respectively. Upon execution, the `train` function provides two outputs: the trained model and the roc_auc score on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86895ffe-f750-43b1-b1f1-2b895d0f129a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import LightGBM  \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(*, learningRate, numLeaves , n_estimators , train_data=X_train, val_data=y_train):\n",
    "    \"\"\"\n",
    "    This train() function:\n",
    "     - takes hyperparameters as inputs (for tuning later)\n",
    "     - returns the roc_auc score on the validation dataset\n",
    "\n",
    "    Wrapping code as a function makes it easier to reuse the code later for tuning.\n",
    "    \"\"\" \n",
    "\n",
    "    # Create a LightGBM classifier with the given hyperparameters and target column\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        learningRate=learningRate,\n",
    "        numLeaves=numLeaves,\n",
    "        labelCol=\"Exited\",\n",
    "        n_estimators =n_estimators ,\n",
    "        verbosity = -1,\n",
    "        random_state= 41\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    lgbm_model.fit(X_train, y_train) \n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # Compute ROC AUC score on train and test\n",
    "    roc_auc_lgbm_train = roc_auc_score(y_train, lgbm_model.predict_proba(X_train)[:, 1])\n",
    "    roc_auc_lgbm_test = roc_auc_score(y_test, lgbm_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    # Log ROC AUC score\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_lgbm_train)\n",
    "    mlflow.log_metric(\"roc_auc_test\", roc_auc_lgbm_test)\n",
    "\n",
    "\n",
    "    # Return the model and the roc_auc score\n",
    "    return lgbm_model, roc_auc_lgbm_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0417a2-3002-4a79-8005-bb68a22d9739",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define ```tune``` function\n",
    "\n",
    "To utilize FLAML, you'll need to define a ```tune``` function. This function takes a ```config``` dictionary as input and returns a dictionary where the evaluation metric serves as the key, and its corresponding value represents the metric's value.\n",
    "\n",
    "The config dictionary includes the hyperparameters you intend to tune along with their respective values. Subsequently, the tune function employs the previously defined train function to train and evaluate the model based on the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dda96-c0a0-49e7-8f6c-07683161012d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import FLAML\n",
    "import flaml\n",
    "\n",
    "# Define the tune function\n",
    "def flaml_tune(config):\n",
    "    # Train and evaluate the model with the given config\n",
    "    model, metric = train(**config)\n",
    "    \n",
    "    # Return the evaluation metric and its value\n",
    "    return {\"roc_auc\": metric}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6fa9c-ffa3-4f58-b4b5-46a37201c3bd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define search space\n",
    "\n",
    "You then need to define the search space for the hyperparameters you want to tune. The search space is a dictionary that maps the hyperparameter names to the ranges of values you want to explore. FLAML offers convenient functions for defining various types of ranges, including uniform, loguniform, and randint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6b5d3-9ff3-4468-b0ac-f9b1f0afde0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "\n",
    "params = {\n",
    "    # Learning rate is a continuous value between 0.001 and 1\n",
    "    \"learningRate\": flaml.tune.uniform(0.001, 1),\n",
    "    # Number of leaves is an integer value between 1 and 10\n",
    "    \"numLeaves\": flaml.tune.randint(1, 10),\n",
    "    # Number of iterations is an integer value between 10 and 50\n",
    "    \"n_estimators\": flaml.tune.randint(10, 50),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe698b0-619d-4759-a200-ca31d8d3a4ca",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define hyperparameter trial\n",
    "\n",
    "Finally, you need to define a hyperparameter trial that will utilize FLAML to optimize the hyperparameters. You'll need to pass the tune function, the search space, the time budget, the number of samples, the metric name, the mode, and the verbosity level to the `flaml.tune.run` function. Additionally, you should start a nested MLflow run to track the results of the trial.\n",
    "\n",
    "You can also explore the results of your hyperparameter trial from you ML Experiment. You can navigate to here by clicking **tune_trial**.\n",
    "![image-alt-text](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/ExploreTuneTrialInExperiment.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d325c4-6130-4ae3-8793-67a2023e47ff",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Start a nested MLflow run\n",
    "with mlflow.start_run(run_name=\"tune_trial\"):\n",
    "    # Run the hyperparameter trial with FLAML\n",
    "    analysis = flaml.tune.run(\n",
    "        # Pass the tune function\n",
    "        flaml_tune,\n",
    "        # Pass the search space\n",
    "        params,\n",
    "        # Set the time budget to 200 seconds\n",
    "        time_budget_s=200,\n",
    "        # Set the metric name \n",
    "        metric=\"roc_auc\",\n",
    "        # Set the mode to max (we want to maximize the roc_auc score)\n",
    "        mode=\"max\",\n",
    "        # Set the verbosity level to -1\n",
    "        verbose=-1,\n",
    "        # Maximal number of configs to try\n",
    "        num_samples=10,\n",
    "        # Use Spark to parallelize the tuning\n",
    "        use_spark= True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd66d8-1b7b-4ac2-a0e4-d58a1665f724",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### View the performance of hyperparameter trial\n",
    "\n",
    "A parallel coordinate plot visually represents hyperparameter trials in machine learning. Each line on the plot corresponds to a different hyperparameter combination, with axes representing individual hyperparameters. \n",
    "\n",
    "This visualization allows for quick identification of patterns and relationships between hyperparameters and model performance. It aids in pinpointing optimal configurations by highlighting areas where the model performs best, facilitating informed decision-making during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81948b8-b5eb-40d1-85cc-2f51f4a1722d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import flaml.visualization as fviz\n",
    "\n",
    "# pio.renderers.default = \"sphinx_gallery\"\n",
    "\n",
    "fig = fviz.plot_parallel_coordinate(analysis, params=[\"learningRate\", \"numLeaves\", \"n_estimators\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba12e9a-5792-4ff3-b714-e9da20fe6184",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get the best config from the analysis object\n",
    "flaml_config = analysis.best_config\n",
    "# Print the best config\n",
    "print(\"Best config: \", flaml_config)\n",
    "print(\"Best score on training data: \", analysis.best_result[\"roc_auc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3743f67-12cb-4070-84dc-e15fdc09ab62",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 5: Compare and save the final results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0fa6d-f6d1-40de-a3a8-ddf60a4ea919",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Compare results\n",
    "\n",
    "After finding the best hyperparameters with FLAML, you need to evaluate how much they improve the model performance. To accomplish this, use the train function to create a new model with the best hyperparameters on the full training dataset. Then, utilize the `test` dataset to calculate the roc_auc score for both the new model and the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2583cb-3c10-47e3-b693-c18c300fd35c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train a new model with the best hyperparameters using FLAML\n",
    "with mlflow.start_run() as run:\n",
    "    flaml_model, flaml_metric = train(**flaml_config, train_data=X_train, val_data=y_train)\n",
    "    \n",
    "    # Infer signature for the model\n",
    "    signature = infer_signature(X_train, flaml_model.predict(X_train))\n",
    "    \n",
    "    # Log FLAML model and its metric\n",
    "    mlflow.log_metric(\"roc_auc\", flaml_metric)\n",
    "    mlflow.lightgbm.log_model(flaml_model, \"model\", signature=signature)\n",
    "    flaml_run_id = run.info.run_id\n",
    "\n",
    "# Compute ROC AUC score for the baseline model on the test dataset\n",
    "roc_auc_lgbm_test = roc_auc_score(y_test, default_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Compute ROC AUC score for the FLAML-tuned model on the test dataset\n",
    "flaml_metric_test = roc_auc_score(y_test, flaml_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print ROC AUC scores for comparison\n",
    "print(\"Analyzing results on the test data\")\n",
    "print(\"On the test dataset, the initial (untuned) model achieved ROC AUC: \", roc_auc_lgbm_test)\n",
    "print(\"On the test dataset, the final FLAML (tuned) model achieved ROC AUC: \", flaml_metric_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc63ba-3bb0-46bc-822a-e78db117e247",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save the final and tuned model\n",
    "\n",
    "Once you have completed your hyperparameter trial, you can now save the final, tuned model as an ML model in Fabric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fd591-de63-4bc2-82f4-76c1967982c5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # Specify the model name and the path where you want to save it in the registry\n",
    "model_name = \"fabcon-churn-model\"  # Replace with your desired model name\n",
    "model_path = f\"runs:/{flaml_run_id}/model\"\n",
    "\n",
    "# Register the model to the MLflow registry\n",
    "registered_model = mlflow.register_model(model_uri=model_path, name=model_name)\n",
    "\n",
    "# Print the registered model's name and version\n",
    "print(f\"Model '{registered_model.name}' version {registered_model.version} registered successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ae38b3-3f87-4b3c-bb85-a6f3b125fdae",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Exercise: Hyperparameter tuning practice\n",
    "\n",
    "In this exercise, you will create a new search space to explore a new set of hyperparameters. \n",
    "\n",
    "Here is a list of the main LightGBM parameters:\n",
    "\n",
    "```python\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Instantiate an LGBMClassifier with a selection of main parameters\n",
    "lgbm_classifier = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', # Type of algorithm to use. Default is 'gbdt' (Gradient Boosting Decision Tree).\n",
    "    num_leaves=31, # Maximum tree leaves for base learners. \n",
    "    max_depth=-1, # Maximum tree depth for base learners, <=0 means no limit.\n",
    "    learning_rate=0.1, # Boosting learning rate.\n",
    "    n_estimators=100, # Number of boosted trees to fit.\n",
    "    subsample_for_bin=200000, # Number of samples for constructing bins.\n",
    "    objective=None, # Specify the learning task and the corresponding learning objective or a custom objective function to be used.\n",
    "    class_weight=None, # Weights associated with classes in the form {class_label: weight}.\n",
    "    min_split_gain=0., # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "    min_child_weight=0.001, # Minimum sum of instance weight (hessian) needed in a child (leaf).\n",
    "    min_child_samples=20, # Minimum number of data needed in a child (leaf).\n",
    "    subsample=1., # Subsample ratio of the training instance.\n",
    "    subsample_freq=0, # Frequency of subsample, <=0 means no enable.\n",
    "    colsample_bytree=1., # Subsample ratio of columns when constructing each tree.\n",
    "    reg_alpha=0., # L1 regularization term on weights.\n",
    "    reg_lambda=0., # L2 regularization term on weights.\n",
    "    random_state=41, # Random number seed.\n",
    "    n_jobs=-1, # Number of parallel threads.\n",
    "    importance_type='split' # Type of feature importance to be filled into `feature_importances_`.\n",
    ")\n",
    "\n",
    "# Note: This is just a model instantiation. You would need to train this model using .fit()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721092b5-3aeb-4d0b-bb38-c0b6be5f0281",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Your default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c11fa-5117-424e-ab5e-bab30066d47a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Here is our default model\n",
    "default_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df9187-d789-4a54-b48b-a2c468206049",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The ROC_AUC score of our baseline model on training data was:  '{roc_auc_default}'.\")\n",
    "print(f\"The ROC_AUC score of our baseline model on test data was:  '{roc_auc_lgbm_test}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ef54e-6b28-4577-8ba8-fd85aae3aa81",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# New model\n",
    "\n",
    "Now, we will explore some new hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdad171-8baa-4a71-85e2-97dc54a6124a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Here, we have our updated training function\n",
    "def new_train(*, num_leaves, max_depth , min_child_samples , train_data=X_train, val_data=y_train):\n",
    "    \"\"\"\n",
    "    This new_train() function:\n",
    "     - takes hyperparameters as inputs (for tuning later)\n",
    "     - returns the roc_auc score on the validation dataset\n",
    "\n",
    "    Wrapping code as a function makes it easier to reuse the code later for tuning.\n",
    "    \"\"\" \n",
    "\n",
    "    # Create a LightGBM classifier with the given hyperparameters and target column\n",
    "    new_model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        max_depth=max_depth, # Maximum tree depth for base learners\n",
    "        num_leaves=num_leaves, # Maximum tree leaves for base learners\n",
    "        labelCol=\"Exited\",\n",
    "        min_child_samples = min_child_samples, # Minimum number of data needed in a child (leaf)\n",
    "        verbosity = -1,\n",
    "        random_state= 41\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    new_model.fit(X_train, y_train) \n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred_new = new_model.predict(X_test)\n",
    "    \n",
    "    # Compute accuracy score\n",
    "    accuracy_new = accuracy_score(y_test, y_pred_new)\n",
    "\n",
    "    # Compute ROC AUC score\n",
    "    roc_auc_lgbm_new = roc_auc_score(y_train, new_model.predict_proba(X_train)[:, 1])\n",
    "    roc_auc_lgbm_test_new = roc_auc_score(y_test, new_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "\n",
    "    # Log ROC AUC score\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_lgbm_new)\n",
    "    mlflow.log_metric(\"roc_auc_test\", roc_auc_lgbm_test_new)\n",
    "\n",
    "\n",
    "    # Return the model and the roc_auc score\n",
    "    return new_model, roc_auc_lgbm_new\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a35582-95b0-4729-a61e-f61fb9c5e97b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import FLAML\n",
    "import flaml\n",
    "\n",
    "# Define the tune function\n",
    "def flaml_tune_new(config):\n",
    "    # Train and evaluate the model with the given config\n",
    "    model, metric = new_train(**config)\n",
    "    \n",
    "    # Return the evaluation metric and its value\n",
    "    return {\"roc_auc\": metric}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd2b6d-8fa3-448a-a76e-8746a1c4541c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### TODO: Customize your search space\n",
    "\n",
    "When defining your search space, you will provide the parameter and domain. In this exercise, you will define search spaces for the following hyperparameters: `num_leaves`, `max_depth`, `min_child_samples`.\n",
    "\n",
    "See the example below for the commonly used types of domains. [Learn more](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#search-space).\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    # Sample a float uniformly between -5.0 and -1.0\n",
    "    \"uniform\": tune.uniform(-5, -1),\n",
    "\n",
    "    # Sample a float uniformly between 3.2 and 5.4,\n",
    "    # rounding to increments of 0.2\n",
    "    \"quniform\": tune.quniform(3.2, 5.4, 0.2),\n",
    "\n",
    "    # Sample a float uniformly between 0.0001 and 0.01, while\n",
    "    # sampling in log space\n",
    "    \"loguniform\": tune.loguniform(1e-4, 1e-2),\n",
    "\n",
    "    # Sample a float uniformly between 0.0001 and 0.1, while\n",
    "    # sampling in log space and rounding to increments of 0.00005\n",
    "    \"qloguniform\": tune.qloguniform(1e-4, 1e-1, 5e-5),\n",
    "\n",
    "    # Sample a random float from a normal distribution with\n",
    "    # mean=10 and sd=2\n",
    "    \"randn\": tune.randn(10, 2),\n",
    "\n",
    "    # Sample a random float from a normal distribution with\n",
    "    # mean=10 and sd=2, rounding to increments of 0.2\n",
    "    \"qrandn\": tune.qrandn(10, 2, 0.2),\n",
    "\n",
    "    # Sample a integer uniformly between -9 (inclusive) and 15 (exclusive)\n",
    "    \"randint\": tune.randint(-9, 15),\n",
    "\n",
    "    # Sample a random uniformly between -21 (inclusive) and 12 (inclusive (!))\n",
    "    # rounding to increments of 3 (includes 12)\n",
    "    \"qrandint\": tune.qrandint(-21, 12, 3),\n",
    "\n",
    "    # Sample a integer uniformly between 1 (inclusive) and 10 (exclusive),\n",
    "    # while sampling in log space\n",
    "    \"lograndint\": tune.lograndint(1, 10),\n",
    "\n",
    "    # Sample a integer uniformly between 2 (inclusive) and 10 (inclusive (!)),\n",
    "    # while sampling in log space and rounding to increments of 2\n",
    "    \"qlograndint\": tune.qlograndint(2, 10, 2),\n",
    "\n",
    "    # Sample an option uniformly from the specified choices\n",
    "    \"choice\": tune.choice([\"a\", \"b\", \"c\"]),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f7ec9-f1d7-4cfb-875b-cc134a97e1e2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Define the new search space\n",
    "\n",
    "params_new = {\n",
    "    # TODO: Provide a range of values for max_depth\n",
    "    \"max_depth\": # TODO,\n",
    "    # TODO: Provide a range of integer values \n",
    "    \"num_leaves\": #TODO,\n",
    "    # TODO: Provide a range of integer values for the min_child_samples\n",
    "    \"min_child_samples\": #TODO,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed5df9-a826-4895-99c6-2c4dd4c9a1d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Disable MLflow autologging to leverage FLAML tune's built-in logging\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efa81d-cbf3-4d83-be39-745a1ee959a5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Start a nested MLflow run\n",
    "with mlflow.start_run(run_name=\"tune_trial_updated\"):\n",
    "    # Run the hyperparameter trial with FLAML\n",
    "    analysis_new = flaml.tune.run(\n",
    "        # Pass the tune function\n",
    "        flaml_tune_new,\n",
    "        # Pass the search space\n",
    "        params_new,\n",
    "        # Set the time budget to 200 seconds\n",
    "        time_budget_s=200,\n",
    "        # Set the metric name \n",
    "        metric=\"roc_auc\",\n",
    "        # Set the mode to max (we want to maximize the roc_auc score)\n",
    "        mode=\"max\",\n",
    "        # Set the verbosity level to -1\n",
    "        verbose=-1,\n",
    "        # Maximal number of configs to try\n",
    "        num_samples=10,\n",
    "        # Use Spark to parallelize the tuning\n",
    "        use_spark= True\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5,
 "notebookName": "4B_Tune_EXERCISE"
}
