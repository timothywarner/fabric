{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70afb0ca-7c34-4878-857e-dfed22fa32bf",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# MLflow overview\n",
    "\n",
    "MLflow is an open-source platform designed for managing the entire machine learning lifecycle, including experimentation, deployment, and model registry. It enables tracking of experiments, comparison of results, and sharing across teams by logging parameters, metrics, and outputs. Its flexibility and comprehensive toolset make MLflow essential for data scientists and developers aiming to streamline the development, deployment, and maintenance of machine learning models.\n",
    "\n",
    "### Exercise overview\n",
    "\n",
    "In this exercise, we will use `churn_data_clean` to train several baseline models. We will track the results of these iterations with MLFlow and learn how we can use autologging to customize the details tracked.\n",
    "\n",
    "### Helpful links\n",
    "- [Autologging in Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/data-science/mlflow-autologging)\n",
    "- [Fabric Experiments](https://learn.microsoft.com/en-us/fabric/data-science/machine-learning-experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd2393d-3920-4f0c-ba9c-ce720f870073",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 1: Read cleaned data from the lakehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb34053-f44f-4d96-b922-1091cd381636",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM FC_Workshop.churn_data_clean\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226cb6c-0087-453c-bc6d-6f5f32fa023e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 2: Prepare datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a6d5f-d78a-4162-b4e7-7f4d47e572d3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Generate train-test datasets \n",
    "\n",
    "The code snippet illustrates the process of preparing a dataset for machine learning model training and evaluation using Scikit-learn and Pandas. Initially, it converts a Spark DataFrame (`df`) into a Pandas DataFrame (`df_final_pd`) to utilize familiar data manipulation operations. It then identifies the target variable (`y`) as the \"Exited\" column and separates the features (`X`) by removing the target column from the dataset. With the features and target defined, it employs the `train_test_split` function from Scikit-learn to divide the dataset into training and testing sets, allocating 20% of the data for testing. This split is controlled by a specified `random_state` to ensure reproducibility of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6b7a8-3fc5-4da7-a0e5-0e5da0156037",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "df_final_pd = df.toPandas()\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "y = df_final_pd[\"Exited\"]\n",
    "X = df_final_pd.drop(\"Exited\", axis=1)\n",
    "random_state = 41\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169eda7-7af0-429f-9aef-985929e6f967",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Save test data\n",
    "\n",
    "This code snippet demonstrates how to save test data for future predictions after processing. It first converts the test dataset (`X_test`), originally in a Pandas DataFrame format, back into a Spark DataFrame (`spark_df`) using the `createDataFrame` method. \n",
    "\n",
    "The snippet then proceeds to save this Spark DataFrame to a designated location specified by `Tables/churn_test_data`. The data is saved in Delta format, a storage layer that brings ACID transactions to Apache Spark and big data workloads, with the `mode` set to \"overwrite\" to ensure that any existing data in the specified path is replaced. This step is crucial for preserving the test set in a reliable and efficient format for later use in making predictions or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081cc8d-fc48-425c-9eae-19d69e91a9c8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save test data for predictions later\n",
    "\n",
    "spark_df = spark.createDataFrame(X_test)\n",
    "spark_df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/churn_test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d22831-0e0d-4cfb-b6b5-70a59745beb1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Step 3: Train baseline models \n",
    "\n",
    "#### Tree based models\n",
    "\n",
    "There are many different hyperparameters that can be tuned for tree based models. In the training exercises, we will experiment with the hyperparameters that impact:\n",
    "- Tree Shape — ```num_leaves``` and ```max_depth```\n",
    "- Tree Growth — ```min_data_in_leaf``` and ```min_gain_to_split```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbc7c0-5cdd-47c1-af87-0c72ec0fbad3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create a machine learning experiment\n",
    "\n",
    "A machine learning experiment is the primary unit of organization and control for all related machine learning runs. A run corresponds to a single execution of model code. In MLflow, tracking is based on experiments and runs. You can tracks runs and the associated information using the inline MLflow widget or by using the Experiment item in Fabric. \n",
    "\n",
    "![Navigate to an ML Experiment in Fabric](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/experiment-details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4b9fd-7ae4-49a6-92c3-eeb4a9363fdf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set the MLflow experiment \n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"FabCon-Demo-Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6806cb2-b309-47ba-b14e-f3e591709ff2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Set the logging level\n",
    "\n",
    "You can configure the logging level to suppress unnecessary outputs from the SynapseML library to keep the logs cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2890d0-aaca-4e5d-989d-1233387c609c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger('synapse.ml').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('mlflow.utils').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a79a03-61d6-4652-bb75-e83b9c0087de",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Autologging\n",
    "\n",
    "Synapse Data Science in Microsoft Fabric includes autologging, which significantly reduces the amount of code required to automatically log the parameters, metrics, and items of a machine learning model during training. This feature extends [MLflow autologging](https://mlflow.org/docs/latest/tracking.html#automatic-logging) capabilities and is deeply integrated into the Synapse Data Science in Microsoft Fabric experience. Using autologging, developers and data scientists can easily track and compare the performance of different models and experiments without the need for manual tracking.\n",
    "\n",
    "Autologging works by automatically capturing the values of input parameters, output metrics, and output items of a machine learning model as it is being trained. This information is then logged to your Microsoft Fabric workspace, where it can be accessed and visualized using the MLflow APIs or the corresponding experiment & model items in your Microsoft Fabric workspace.\n",
    "\n",
    "```python\n",
    "mlflow.autolog(\n",
    "    log_input_examples=False,\n",
    "    log_model_signatures=True,\n",
    "    log_models=True,\n",
    "    disable=False,\n",
    "    exclusive=True,\n",
    "    disable_for_unsupported_versions=True,\n",
    "    silent=True)\n",
    "```\n",
    "\n",
    "When you launch a Synapse Data Science notebook, Microsoft Fabric calls ```mlflow.autolog()``` to instantly enable the tracking and load the corresponding dependencies. As you train models in your notebook, this model information is automatically tracked with MLflow. This configuration is done automatically behind the scenes when you run import mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c610d-369c-4085-9e08-a7a782e02fb9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Mode 1: Enable Autologging\n",
    "\n",
    "In Fabric workspaces, autologging is activated by default. After it has been executed, you have the ability to review the logged parameters and metrics. It's important to note that these details were logged automatically, without the need for manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffe8b7-b50d-4406-9db5-c877d02377c7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This is enabled by default, but you can also call the following command to re-enable with the original settings\n",
    "\n",
    "# Use this\n",
    "# mlflow.autolog()\n",
    "\n",
    "# or this \n",
    "\n",
    "# mlflow.autolog(\n",
    "#     log_input_examples=False,\n",
    "#     log_model_signatures=True,\n",
    "#     log_models=True,\n",
    "#     disable=False,\n",
    "#     exclusive=True,\n",
    "#     disable_for_unsupported_versions=True,\n",
    "#     silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ec338-f85b-48d2-abe2-68fe010f3f27",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import mlflow\n",
    "\n",
    "random_state = 41  \n",
    "\n",
    "with mlflow.start_run(run_name=\"decision_tree_default\") as run:\n",
    "    \n",
    "    # Define DecisionTreeClassifier with specified parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        max_depth=2,  \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54352dd5-b690-4efd-a260-1251b28bbcb2",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Mode 2: Disable Autologging\n",
    "\n",
    "To disable Microsoft Fabric autologging in a notebook session, you can call ```mlflow.autolog()``` and set ```disable=True```. This will require you to manually log any metrics, files, or parameters that you want logged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3af7c-43e6-4dc3-9c62-9b7b15f0b94b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Disable autologging\n",
    "mlflow.autolog(disable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c56b66-03c6-4d2b-94aa-dc8df4c5521e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import mlflow\n",
    "\n",
    "random_state = 41 \n",
    "\n",
    "with mlflow.start_run(run_name=\"dt_autolog_disabled\") as run:\n",
    "    \n",
    "    # Define DecisionTreeClassifier with specified parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        max_depth=2,  \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f50c1-cd80-4d43-a705-996edba1ea2a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Mode 3: Custom logging\n",
    "\n",
    "There are scenarios where you'll want to review the automatically logged metrics, parameters, or files, but also log your own custom metrics or metadata. To accommodate this, you can disable the exclusive autologging mode by setting it to ```False```. Doing so enables you to monitor both the properties automatically captured by MLflow and those manually logged by you.\n",
    "\n",
    "Here's an example on how  you can enable and use custom logging:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "mlflow.autolog(exclusive=False)\n",
    "\n",
    "with mlflow.start_run():\n",
    "  mlflow.log_param(\"parameter name\", \"example value\")\n",
    "  # <add model training code here>\n",
    "  mlflow.log_metric(\"metric name\", 20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b53f4-cd1c-4f83-9b2f-f195dff0a263",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(exclusive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb0456-5103-4cbd-bd28-ff9ae4e9de58",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "\n",
    "random_state = 41\n",
    "\n",
    "with mlflow.start_run(run_name=\"dt_autolog_custom\") as run:\n",
    "    \n",
    "    # Define DecisionTreeClassifier with specified parameters\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        max_depth=2,  \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"autolog_mode\", \"custom\")\n",
    "\n",
    "    # Generate probability scores for the positive class\n",
    "    y_proba = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    # Log the ROC AUC score\n",
    "    mlflow.log_metric(\"roc_auc_test\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4f507-dba7-4b38-a1e6-bbdf550b78d8",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Exercise 1: Train a baseline LightGBM model\n",
    "\n",
    "Next, we'll leverage the exclusive autologging feature to train our initial LightGBM model. Each model type records a unique set of information through autologging. By consulting the [MLflow documentation](https://mlflow.org/docs/2.4.2/tracking.html#lightgbm), we can observe that the following specifics are automatically documented for autologging:\n",
    "\n",
    "![MLFlow Docs for LightGBM](https://synapseaisolutionsa.blob.core.windows.net/public/Fabric-Conference/lgbm-autolog.png)\n",
    "\n",
    "### To do\n",
    "In this exercise, you will add additional code to: \n",
    "- Complete the TODO items below. You will need to calculate Accuracy and ROC_AUC score on the X_train dataset\n",
    "- Log these new metrics using ```mlflow.log_metrics```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545bf71-be55-462e-914d-7f0a6c494b9e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"default_lgbm\") as run:\n",
    "\n",
    "    # Define LGBMClassifier with specified parameters\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        learning_rate=0.01,\n",
    "        n_estimators=2,\n",
    "        max_depth=2,\n",
    "        num_leaves=3,\n",
    "        objective='binary',\n",
    "        random_state=random_state,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    # Capture run_id for model prediction later\n",
    "    lgbm_model_run_id = run.info.run_id \n",
    "\n",
    "    # Fit the model to the training data\n",
    "    lgbm_model.fit(X_train, y_train) \n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    \n",
    "    # TODO: Compute accuracy score\n",
    "\n",
    "    # TODO: Compute ROC AUC score\n",
    "\n",
    "    # TODO: Log all metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f1455-0a2a-48ea-b071-050ec921f916",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Save the final model\n",
    "\n",
    "A machine learning model is a file trained to recognize certain types of patterns. You train a model over a set of data, and you provide it with an algorithm that uses to reason over and learn from that data set. After you train the model, you can use it to reason over data that it never saw before, and make predictions about that data.\n",
    "\n",
    "In MLflow, a machine learning model can include multiple model versions. Here, each version can represent a model iteration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b4f2b-a368-4c71-a3c8-8dc068c9cd61",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Specify the model name and the path where you want to save it in the registry\n",
    "model_name = \"fabcon-churn-model\"  # Replace with your desired model name\n",
    "model_path = f\"runs:/{lgbm_model_run_id}/model\"\n",
    "\n",
    "# Register the model to the MLflow registry\n",
    "registered_model = mlflow.register_model(model_uri=model_path, name=model_name)\n",
    "\n",
    "# Print the registered model's name and version\n",
    "print(f\"Model '{registered_model.name}' version {registered_model.version} registered successfully.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
