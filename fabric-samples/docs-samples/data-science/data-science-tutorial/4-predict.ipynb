{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3678236-c580-4d8e-9e77-8c2d8eac0a40",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Part 4: Score the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07c860-f226-4090-bfaf-1d57aaab5eec",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "Microsoft Fabric allows you to operationalize machine learning models with a scalable function called PREDICT, which supports batch scoring in any compute engine. You can generate batch predictions directly from a Microsoft Fabric notebook or from a given model's item page. Learn about [PREDICT](https://aka.ms/fabric-predict).  \n",
    "\n",
    "To generate batch predictions on our test dataset, you'll use version 1 of the trained churn model. You'll load the test dataset into a spark DataFrame and create an MLFlowTransformer object to generate batch predictions. You can then invoke the PREDICT function using one of following three ways: \n",
    "\n",
    "- Using the Transformer API from SynapseML\n",
    "- Using the Spark SQL API\n",
    "- Using PySpark user-defined function (UDF)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Complete [Part 3: Train and register machine learning models](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-train-models).\n",
    "- Attach the same lakehouse you used in Part 3 to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4283b7b",
   "metadata": {},
   "source": [
    "## Load the test data\n",
    "\n",
    "Load the test data that you saved in Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6842103-e249-4580-ae7c-68cb330df2e1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.format(\"delta\").load(\"Tables/df_test\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e6b31-3262-4246-809a-5294ef898243",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### PREDICT with the Transformer API\n",
    "\n",
    "To use the Transformer API from SynapseML, you'll need to first create an MLFlowTransformer object.\n",
    "\n",
    "### Instantiate MLFlowTransformer object\n",
    "\n",
    "The MLFlowTransformer object is a wrapper around the MLFlow model that you registered in Part 3. It allows you to generate batch predictions on a given DataFrame. To instantiate the MLFlowTransformer object, you'll need to provide the following parameters:\n",
    "\n",
    "- The columns from the test DataFrame that you need as input to the model (in this case, you would need all of them).\n",
    "- A name for the new output column (in this case, predictions).\n",
    "- The correct model name and model version to generate the predictions (in this case, `lgbm_sm` and version 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77804ed-6d08-40a9-a406-894ccfa4ae93",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.predict import MLFlowTransformer\n",
    "\n",
    "model = MLFlowTransformer(\n",
    "    inputCols=list(df_test.columns),\n",
    "    outputCol='predictions',\n",
    "    modelName='lgbm_sm',\n",
    "    modelVersion=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29195965",
   "metadata": {},
   "source": [
    "Now that you have the MLFlowTransformer object, you can use it to generate batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1542e-5174-4f53-a97a-94ae8df8a649",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "predictions = model.transform(df_test)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baae42-6fd2-43c4-b27f-88dc4a280852",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### PREDICT with the Spark SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b4f7f-788d-40d1-ad28-7576e91533f3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer \n",
    "\n",
    "# Substitute \"model_name\", \"model_version\", and \"features\" below with values for your own model name, model version, and feature columns\n",
    "model_name = 'lgbm_sm'\n",
    "model_version = 1\n",
    "features = df_test.columns\n",
    "\n",
    "sqlt = SQLTransformer().setStatement( \n",
    "    f\"SELECT PREDICT('{model_name}/{model_version}', {','.join(features)}) as predictions FROM __THIS__\")\n",
    "\n",
    "# Substitute \"X_test\" below with your own test dataset\n",
    "display(sqlt.transform(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919e3c2-dfb3-42e1-972a-286b628bb5ef",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### PREDICT with a user-defined function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ac80f-1395-4c2c-83a0-fc2e78f48b60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, udf, lit\n",
    "\n",
    "# Substitute \"model\" and \"features\" below with values for your own model name and feature columns\n",
    "my_udf = model.to_udf()\n",
    "features = df_test.columns\n",
    "\n",
    "display(df_test.withColumn(\"predictions\", my_udf(*[col(f) for f in features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d20c2-fa5c-469e-abea-1e53fb85e95f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Write model prediction results to the lakehouse\n",
    "\n",
    "Once you have generated batch predictions, write the model prediction results back to the lakehouse.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdaa6e-d02b-45e7-9673-dba74e3df738",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions to lakehouse to be used for generating a Power BI report\n",
    "table_name = \"customer_churn_test_predictions\"\n",
    "predictions.write.format('delta').mode(\"overwrite\").save(f\"Tables/{table_name}\")\n",
    "print(f\"Spark DataFrame saved to delta table: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9d2b8",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "Use these predictions you just saved to [create a report in Power BI](https://learn.microsoft.com/fabric/data-science/tutorial-data-science-create-report)."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
