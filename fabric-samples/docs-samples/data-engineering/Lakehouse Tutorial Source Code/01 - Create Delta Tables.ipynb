{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "### Spark session configuration\r\n",
                "This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Copyright (c) Microsoft Corporation.\r\n",
                "# Licensed under the MIT License.\r\n",
                "\r\n",
                "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\r\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\r\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": null,
                            "statement_id": null,
                            "state": "waiting",
                            "livy_statement_state": null,
                            "queued_time": "2023-04-14T17:53:12.0196844Z",
                            "session_start_time": null,
                            "execution_start_time": null,
                            "execution_finish_time": null,
                            "spark_jobs": null,
                            "parent_msg_id": "a019e3c3-bf39-4a15-b48f-7ff06e3f53c8"
                        },
                        "text/plain": "StatementMeta(, , , Waiting, )"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 1,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0196844Z\",\"session_start_time\":\"2023-04-14T17:53:12.2618501Z\",\"execution_start_time\":\"2023-04-14T17:53:22.8437076Z\",\"execution_finish_time\":\"2023-04-14T17:53:24.8678387Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Fact - Sale\r\n",
                "\r\n",
                "This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.functions import col, year, month, quarter\r\n",
                "\r\n",
                "table_name = 'fact_sale'\r\n",
                "\r\n",
                "df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/fact_sale_1y_full')\r\n",
                "df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\r\n",
                "df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\r\n",
                "df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\r\n",
                "\r\n",
                "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": null,
                            "statement_id": null,
                            "state": "waiting",
                            "livy_statement_state": null,
                            "queued_time": "2023-04-14T18:02:25.4147613Z",
                            "session_start_time": null,
                            "execution_start_time": null,
                            "execution_finish_time": null,
                            "spark_jobs": null,
                            "parent_msg_id": "78a397fd-d9fa-40d1-8b68-15bfb96f2cc5"
                        },
                        "text/plain": "StatementMeta(, , , Waiting, )"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T18:02:25.4147613Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T18:02:25.768228Z\",\"execution_finish_time\":\"2023-04-14T18:03:21.4120056Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Dimensions\r\n",
                "This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."
            ],
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql.types import *\n",
                "\n",
                "def loadFullDataFromSource(table_name):\n",
                "    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/full/' + table_name)\n",
                "    df = df.select([c for c in df.columns if c != 'Photo'])\n",
                "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n",
                "\n",
                "full_tables = [\n",
                "    'dimension_city',\n",
                "    'dimension_date',\n",
                "    'dimension_employee',\n",
                "    'dimension_stock_item'\n",
                "    ]\n",
                "\n",
                "for table in full_tables:\n",
                "    loadFullDataFromSource(table)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "spark_pool": null,
                            "session_id": null,
                            "statement_id": null,
                            "state": "waiting",
                            "livy_statement_state": null,
                            "queued_time": "2023-04-14T17:53:12.0270275Z",
                            "session_start_time": null,
                            "execution_start_time": null,
                            "execution_finish_time": null,
                            "spark_jobs": null,
                            "parent_msg_id": "e3d41e84-8ba4-4aba-8a8a-671eb7f35859"
                        },
                        "text/plain": "StatementMeta(, , , Waiting, )"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 3,
            "metadata": {
                "jupyter": {
                    "source_hidden": false,
                    "outputs_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                },
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0270275Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T17:56:22.4495972Z\",\"execution_finish_time\":\"2023-04-14T17:56:32.6772207Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}"
            }
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "kernelspec": {
            "name": "synapse_pyspark",
            "display_name": "Synapse PySpark"
        },
        "widgets": {},
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "enableDebugMode": false,
                "conf": {}
            }
        },
        "notebook_environment": {},
        "synapse_widget": {
            "version": "0.1",
            "state": {}
        },
        "trident": {
            "lakehouse": {
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
